{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üöÄ SVM+HOG vs CNN Image Classification Comparison\n",
    "\n",
    "This notebook provides a comprehensive comparison between two different image classification approaches:\n",
    "- **SVM + HOG** (Traditional Computer Vision)\n",
    "- **CNN** (Deep Learning)\n",
    "\n",
    "## üìä Features\n",
    "- Comprehensive accuracy, training time, and inference time analysis\n",
    "- Automatic GPU detection and utilization\n",
    "- Detailed visualizations and confusion matrices\n",
    "- Separate downloads for models, plots, and results\n",
    "- Configurable hyperparameters for both approaches\n",
    "\n",
    "## üéØ Dataset\n",
    "- **Classes**: 3 (normal, cheating, looking_around)\n",
    "- **Images**: 150 total (50 per class)\n",
    "- **Format**: PNG images (128x128)\n",
    "- **Type**: Synthetic dataset with distinct visual patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## üîß Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dependencies"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install opencv-python-headless scikit-image tqdm\n",
    "\n",
    "# Import all required libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import json\n",
    "import zipfile\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# Machine Learning imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from skimage.feature import hog\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "print(\"‚úÖ All dependencies installed and imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpu_setup"
   },
   "source": [
    "## üéÆ GPU Detection and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu_detection"
   },
   "outputs": [],
   "source": [
    "# GPU Detection and Configuration\n",
    "print(\"üîç Checking GPU availability...\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Check GPU availability\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Enable memory growth for GPU\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"üöÄ GPU detected and configured! Available GPUs: {len(gpus)}\")\n",
    "        for i, gpu in enumerate(gpus):\n",
    "            print(f\"   GPU {i}: {gpu.name}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"‚ö†Ô∏è GPU configuration error: {e}\")\nelse:\n",
    "    print(\"üíª No GPU detected. Running on CPU.\")\n",
    "\n",
    "# Check if GPU is being used\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name:\n",
    "    print(f\"üéØ Using GPU: {device_name}\")\n",
    "else:\n",
    "    print(\"üñ•Ô∏è Using CPU for computations\")\n",
    "\n",
    "# Set mixed precision for better GPU performance\n",
    "if gpus:\n",
    "    try:\n",
    "        policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "        tf.keras.mixed_precision.set_global_policy(policy)\n",
    "        print(\"‚ö° Mixed precision enabled for better GPU performance\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Mixed precision not available, using default precision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## ‚öôÔ∏è Configuration Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "configuration"
   },
   "outputs": [],
   "source": [
    "# Configuration Settings\n",
    "# Dataset Configuration\n",
    "DATASET_CONFIG = {\n",
    "    'image_size': (128, 128),\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 42,\n",
    "    'supported_formats': ['.png'],\n",
    "}\n",
    "\n",
    "# SVM + HOG Configuration\n",
    "SVM_HOG_CONFIG = {\n",
    "    'hog_params': {\n",
    "        'orientations': 9,\n",
    "        'pixels_per_cell': (8, 8),\n",
    "        'cells_per_block': (2, 2),\n",
    "        'block_norm': 'L2-Hys',\n",
    "        'visualize': False,\n",
    "    },\n",
    "    'svm_params': {\n",
    "        'C': 1.0,\n",
    "        'kernel': 'rbf',\n",
    "        'gamma': 'scale',\n",
    "        'random_state': 42,\n",
    "        'probability': True,\n",
    "    },\n",
    "    'scaler': 'StandardScaler'\n",
    "}\n",
    "\n",
    "# CNN Configuration\n",
    "CNN_CONFIG = {\n",
    "    'architecture': {\n",
    "        'input_shape': (128, 128, 3),\n",
    "        'conv_layers': [\n",
    "            {'filters': 32, 'kernel_size': (3, 3), 'activation': 'relu'},\n",
    "            {'filters': 64, 'kernel_size': (3, 3), 'activation': 'relu'},\n",
    "            {'filters': 128, 'kernel_size': (3, 3), 'activation': 'relu'},\n",
    "        ],\n",
    "        'dense_layers': [\n",
    "            {'units': 128, 'activation': 'relu', 'dropout': 0.5},\n",
    "            {'units': 64, 'activation': 'relu', 'dropout': 0.3},\n",
    "        ],\n",
    "        'output_activation': 'softmax'\n",
    "    },\n",
    "    'compilation': {\n",
    "        'optimizer': 'adam',\n",
    "        'loss': 'categorical_crossentropy',\n",
    "        'metrics': ['accuracy'],\n",
    "        'learning_rate': 0.001\n",
    "    },\n",
    "    'training': {\n",
    "        'epochs': 50,\n",
    "        'batch_size': 32,\n",
    "        'validation_split': 0.2,\n",
    "        'early_stopping': {\n",
    "            'monitor': 'val_accuracy',\n",
    "            'patience': 10,\n",
    "            'restore_best_weights': True\n",
    "        }\n",
    "    },\n",
    "    'augmentation': {\n",
    "        'rotation_range': 20,\n",
    "        'width_shift_range': 0.2,\n",
    "        'height_shift_range': 0.2,\n",
    "        'shear_range': 0.2,\n",
    "        'zoom_range': 0.2,\n",
    "        'horizontal_flip': True,\n",
    "        'fill_mode': 'nearest'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Evaluation Configuration\n",
    "EVALUATION_CONFIG = {\n",
    "    'metrics': ['accuracy', 'precision', 'recall', 'f1-score'],\n",
    "    'figure_size': (12, 8),\n",
    "    'save_plots': True,\n",
    "}\n",
    "\n",
    "# Timing Configuration\n",
    "TIMING_CONFIG = {\n",
    "    'inference_samples': 100,\n",
    "    'timing_runs': 5,\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuration loaded successfully!\")\n",
    "print(f\"üìä Image size: {DATASET_CONFIG['image_size']}\")\n",
    "print(f\"üß† CNN epochs: {CNN_CONFIG['training']['epochs']}\")\n",
    "print(f\"‚öôÔ∏è SVM kernel: {SVM_HOG_CONFIG['svm_params']['kernel']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset_creation"
   },
   "source": [
    "## üìÅ Dataset Creation and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dataset"
   },
   "outputs": [],
   "source": [
    "# Dataset Creation Function\n",
    "def create_synthetic_dataset():\n",
    "    \"\"\"Create a synthetic dataset with distinct visual patterns for each class\"\"\"\n",
    "    \n",
    "    # Create dataset directory structure\n",
    "    dataset_dir = 'dataset'\n",
    "    classes = ['normal', 'cheating', 'looking_around']\n",
    "    \n",
    "    # Create directories\n",
    "    for class_name in classes:\n",
    "        class_dir = os.path.join(dataset_dir, class_name)\n",
    "        os.makedirs(class_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate sample images for each class\n",
    "    image_size = DATASET_CONFIG['image_size']\n",
    "    num_images_per_class = 50\n",
    "    \n",
    "    for i, class_name in enumerate(classes):\n",
    "        print(f\"Creating {num_images_per_class} sample images for class '{class_name}'...\")\n",
    "        \n",
    "        for j in tqdm(range(num_images_per_class), desc=f\"{class_name}\"):\n",
    "            # Create a synthetic image with different patterns for each class\n",
    "            img = np.zeros((*image_size, 3), dtype=np.uint8)\n",
    "            \n",
    "            if class_name == 'normal':\n",
    "                # Normal: Blue-ish with some noise\n",
    "                img[:, :, 2] = 150 + np.random.randint(0, 50, image_size)  # Blue channel\n",
    "                img[:, :, 1] = 50 + np.random.randint(0, 30, image_size)   # Green channel\n",
    "                img[:, :, 0] = 30 + np.random.randint(0, 20, image_size)   # Red channel\n",
    "                \n",
    "            elif class_name == 'cheating':\n",
    "                # Cheating: Red-ish with specific patterns\n",
    "                img[:, :, 0] = 150 + np.random.randint(0, 50, image_size)  # Red channel\n",
    "                img[:, :, 1] = 30 + np.random.randint(0, 20, image_size)   # Green channel\n",
    "                img[:, :, 2] = 30 + np.random.randint(0, 20, image_size)   # Blue channel\n",
    "                \n",
    "                # Add some diagonal patterns\n",
    "                for k in range(0, image_size[0], 10):\n",
    "                    img[k:k+2, :, :] = 255\n",
    "                \n",
    "            elif class_name == 'looking_around':\n",
    "                # Looking around: Green-ish with circular patterns\n",
    "                img[:, :, 1] = 150 + np.random.randint(0, 50, image_size)  # Green channel\n",
    "                img[:, :, 0] = 30 + np.random.randint(0, 20, image_size)   # Red channel\n",
    "                img[:, :, 2] = 30 + np.random.randint(0, 20, image_size)   # Blue channel\n",
    "                \n",
    "                # Add some circular patterns\n",
    "                center = (image_size[0] // 2, image_size[1] // 2)\n",
    "                for radius in range(10, 50, 10):\n",
    "                    for angle in range(0, 360, 10):\n",
    "                        x = int(center[0] + radius * np.cos(np.radians(angle)))\n",
    "                        y = int(center[1] + radius * np.sin(np.radians(angle)))\n",
    "                        if 0 <= x < image_size[0] and 0 <= y < image_size[1]:\n",
    "                            img[x-1:x+2, y-1:y+2, :] = 255\n",
    "            \n",
    "            # Convert to PIL Image and save as PNG\n",
    "            pil_img = Image.fromarray(img)\n",
    "            filename = f\"{class_name}_{j+1:03d}.png\"\n",
    "            filepath = os.path.join(dataset_dir, class_name, filename)\n",
    "            pil_img.save(filepath)\n",
    "    \n",
    "    print(f\"‚úÖ Synthetic dataset created successfully!\")\n",
    "    print(f\"üìÅ Dataset location: {dataset_dir}\")\n",
    "    print(f\"üìä Classes: {classes}\")\n",
    "    print(f\"üñºÔ∏è Images per class: {num_images_per_class}\")\n",
    "    print(f\"üìè Image size: {image_size}\")\n",
    "    \n",
    "    return dataset_dir, classes\n",
    "\n",
    "# Create the dataset\n",
    "dataset_dir, class_names = create_synthetic_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize_dataset"
   },
   "outputs": [],
   "source": [
    "# Visualize Sample Images from Dataset\n",
    "def visualize_sample_images(dataset_dir, class_names, samples_per_class=3):\n",
    "    \"\"\"Visualize sample images from each class\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(len(class_names), samples_per_class, \n",
    "                            figsize=(15, 5 * len(class_names)))\n",
    "    \n",
    "    for i, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(dataset_dir, class_name)\n",
    "        image_files = [f for f in os.listdir(class_dir) if f.endswith('.png')][:samples_per_class]\n",
    "        \n",
    "        for j, img_file in enumerate(image_files):\n",
    "            img_path = os.path.join(class_dir, img_file)\n",
    "            img = Image.open(img_path)\n",
    "            \n",
    "            axes[i, j].imshow(img)\n",
    "            axes[i, j].set_title(f\"{class_name} - Sample {j+1}\")\n",
    "            axes[i, j].axis('off')\n",
    "    \n",
    "    plt.suptitle(\"Sample Images from Each Class\", fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the dataset\n",
    "print(\"üì∏ Visualizing sample images from the dataset...\")\n",
    "visualize_sample_images(dataset_dir, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_loader"
   },
   "source": [
    "## üìä Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_loading"
   },
   "outputs": [],
   "source": [
    "# Data Loading and Preprocessing Class\n",
    "class ImageDataLoader:\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.image_size = DATASET_CONFIG['image_size']\n",
    "        self.test_size = DATASET_CONFIG['test_size']\n",
    "        self.random_state = DATASET_CONFIG['random_state']\n",
    "        self.supported_formats = DATASET_CONFIG['supported_formats']\n",
    "        \n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.class_names = []\n",
    "        \n",
    "    def load_dataset(self):\n",
    "        \"\"\"Load images from subdirectories organized by class\"\"\"\n",
    "        print(\"üìÇ Loading dataset...\")\n",
    "        \n",
    "        if not os.path.exists(self.data_dir):\n",
    "            raise FileNotFoundError(f\"Dataset directory '{self.data_dir}' not found!\")\n",
    "        \n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        # Get class directories\n",
    "        class_dirs = [d for d in os.listdir(self.data_dir) \n",
    "                     if os.path.isdir(os.path.join(self.data_dir, d))]\n",
    "        \n",
    "        if not class_dirs:\n",
    "            raise ValueError(f\"No class directories found in '{self.data_dir}'\")\n",
    "        \n",
    "        self.class_names = sorted(class_dirs)\n",
    "        print(f\"Found {len(self.class_names)} classes: {self.class_names}\")\n",
    "        \n",
    "        # Load images from each class\n",
    "        for class_name in tqdm(self.class_names, desc=\"Loading classes\"):\n",
    "            class_path = os.path.join(self.data_dir, class_name)\n",
    "            \n",
    "            # Get all supported image files\n",
    "            image_files = [f for f in os.listdir(class_path) \n",
    "                          if any(f.lower().endswith(ext) for ext in self.supported_formats)]\n",
    "            \n",
    "            if not image_files:\n",
    "                print(f\"Warning: No supported images found in '{class_path}'\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  - {class_name}: {len(image_files)} images\")\n",
    "            \n",
    "            for img_file in image_files:\n",
    "                img_path = os.path.join(class_path, img_file)\n",
    "                try:\n",
    "                    # Load and preprocess image\n",
    "                    image = self._load_and_preprocess_image(img_path)\n",
    "                    if image is not None:\n",
    "                        images.append(image)\n",
    "                        labels.append(class_name)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {img_path}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        if not images:\n",
    "            raise ValueError(\"No images were successfully loaded!\")\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        X = np.array(images)\n",
    "        y = np.array(labels)\n",
    "        \n",
    "        # Encode labels\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        \n",
    "        print(f\"‚úÖ Dataset loaded: {len(X)} images, {len(self.class_names)} classes\")\n",
    "        print(f\"üìä Image shape: {X.shape}\")\n",
    "        \n",
    "        return X, y_encoded, self.class_names\n",
    "    \n",
    "    def _load_and_preprocess_image(self, img_path):\n",
    "        \"\"\"Load and preprocess a single image\"\"\"\n",
    "        try:\n",
    "            # Load image using PIL\n",
    "            img = Image.open(img_path)\n",
    "            \n",
    "            # Convert to RGB if needed\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            \n",
    "            # Resize image\n",
    "            img = img.resize(self.image_size, Image.Resampling.LANCZOS)\n",
    "            \n",
    "            # Convert to numpy array\n",
    "            img_array = np.array(img)\n",
    "            \n",
    "            # Normalize pixel values to [0, 1]\n",
    "            img_array = img_array.astype(np.float32) / 255.0\n",
    "            \n",
    "            return img_array\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def prepare_data_for_svm(self, X, y):\n",
    "        \"\"\"Prepare data for SVM (flatten images)\"\"\"\n",
    "        # Flatten images for SVM\n",
    "        X_flat = X.reshape(X.shape[0], -1)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_flat, y, test_size=self.test_size, \n",
    "            random_state=self.random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def prepare_data_for_cnn(self, X, y):\n",
    "        \"\"\"Prepare data for CNN (keep image structure)\"\"\"\n",
    "        # Convert labels to categorical\n",
    "        y_categorical = to_categorical(y, num_classes=len(self.class_names))\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y_categorical, test_size=self.test_size, \n",
    "            random_state=self.random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def create_data_generator(self, X_train, y_train, batch_size=32):\n",
    "        \"\"\"Create data generator with augmentation for CNN\"\"\"\n",
    "        datagen = ImageDataGenerator(**CNN_CONFIG['augmentation'])\n",
    "        \n",
    "        # Fit the generator to training data\n",
    "        datagen.fit(X_train)\n",
    "        \n",
    "        # Create generator\n",
    "        generator = datagen.flow(X_train, y_train, batch_size=batch_size)\n",
    "        \n",
    "        return generator\n",
    "    \n",
    "    def get_class_distribution(self, y):\n",
    "        \"\"\"Get class distribution for analysis\"\"\"\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        distribution = dict(zip(unique, counts))\n",
    "        \n",
    "        print(\"\\nüìä Class Distribution:\")\n",
    "        for i, class_name in enumerate(self.class_names):\n",
    "            count = distribution.get(i, 0)\n",
    "            print(f\"  {class_name}: {count} images\")\n",
    "        \n",
    "        return distribution\n",
    "\n",
    "# Load and prepare data\n",
    "print(\"üîÑ Loading and preparing data...\")\n",
    "data_loader = ImageDataLoader(data_dir=dataset_dir)\n",
    "\n",
    "# Load raw data\n",
    "X, y, class_names = data_loader.load_dataset()\n",
    "\n",
    "# Show class distribution\n",
    "data_loader.get_class_distribution(y)\n",
    "\n",
    "# Prepare data for both models\n",
    "X_train_svm, X_test_svm, y_train_svm, y_test_svm = data_loader.prepare_data_for_svm(X, y)\n",
    "X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = data_loader.prepare_data_for_cnn(X, y)\n",
    "\n",
    "print(f\"\\n‚úÖ Data prepared successfully!\")\n",
    "print(f\"   Training samples: {len(X_train_svm)}\")\n",
    "print(f\"   Test samples: {len(X_test_svm)}\")\n",
    "print(f\"   Classes: {len(class_names)}\")\n",
    "print(f\"   Class names: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svm_model"
   },
   "source": [
    "## üîß SVM + HOG Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "svm_hog_model"
   },
   "outputs": [],
   "source": [
    "# SVM + HOG Model Class\n",
    "class SVMHOGModel:\n",
    "    def __init__(self, config=None):\n",
    "        self.config = config or SVM_HOG_CONFIG\n",
    "        self.hog_params = self.config['hog_params']\n",
    "        self.svm_params = self.config['svm_params']\n",
    "        \n",
    "        # Initialize pipeline\n",
    "        self.pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('svm', SVC(**self.svm_params))\n",
    "        ])\n",
    "        \n",
    "        self.is_trained = False\n",
    "        self.training_time = 0\n",
    "        \n",
    "    def extract_hog_features(self, images):\n",
    "        \"\"\"Extract HOG features from images\"\"\"\n",
    "        print(\"üîç Extracting HOG features...\")\n",
    "        \n",
    "        features = []\n",
    "        for img in tqdm(images, desc=\"HOG extraction\"):\n",
    "            try:\n",
    "                # Convert to grayscale if needed for HOG\n",
    "                if len(img.shape) == 3:\n",
    "                    # Convert RGB to grayscale\n",
    "                    gray = np.dot(img[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "                else:\n",
    "                    gray = img\n",
    "                \n",
    "                # Ensure image is valid for HOG\n",
    "                if gray.shape[0] < 32 or gray.shape[1] < 32:\n",
    "                    print(f\"Warning: Image too small for HOG: {gray.shape}\")\n",
    "                    features.append(np.zeros(1296))  # Default HOG feature size\n",
    "                    continue\n",
    "                \n",
    "                # Extract HOG features\n",
    "                hog_features = hog(\n",
    "                    gray,\n",
    "                    orientations=self.hog_params['orientations'],\n",
    "                    pixels_per_cell=self.hog_params['pixels_per_cell'],\n",
    "                    cells_per_block=self.hog_params['cells_per_block'],\n",
    "                    block_norm=self.hog_params['block_norm'],\n",
    "                    visualize=self.hog_params['visualize']\n",
    "                )\n",
    "                \n",
    "                features.append(hog_features)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting HOG features: {e}\")\n",
    "                # Use zero features as fallback\n",
    "                features.append(np.zeros(1296))  # Default HOG feature size\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    def train(self, X_train, y_train):\n",
    "        \"\"\"Train the SVM + HOG model\"\"\"\n",
    "        print(\"üîß Training SVM + HOG model...\")\n",
    "        \n",
    "        # Extract HOG features\n",
    "        X_train_hog = self.extract_hog_features(X_train)\n",
    "        \n",
    "        # Train the pipeline\n",
    "        start_time = time.time()\n",
    "        self.pipeline.fit(X_train_hog, y_train)\n",
    "        self.training_time = time.time() - start_time\n",
    "        \n",
    "        self.is_trained = True\n",
    "        print(f\"‚úÖ Training completed in {self.training_time:.2f} seconds\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained before making predictions\")\n",
    "        \n",
    "        # Extract HOG features\n",
    "        X_test_hog = self.extract_hog_features(X_test)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = self.pipeline.predict(X_test_hog)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict_proba(self, X_test):\n",
    "        \"\"\"Get prediction probabilities\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained before making predictions\")\n",
    "        \n",
    "        # Extract HOG features\n",
    "        X_test_hog = self.extract_hog_features(X_test)\n",
    "        \n",
    "        # Get probabilities\n",
    "        probabilities = self.pipeline.predict_proba(X_test_hog)\n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def evaluate(self, X_test, y_test, class_names):\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        print(\"üìä Evaluating SVM + HOG model...\")\n",
    "        \n",
    "        # Make predictions\n",
    "        start_time = time.time()\n",
    "        predictions = self.predict(X_test)\n",
    "        inference_time = (time.time() - start_time) / len(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        \n",
    "        # Generate classification report\n",
    "        report = classification_report(\n",
    "            y_test, predictions, \n",
    "            target_names=class_names, \n",
    "            output_dict=True\n",
    "        )\n",
    "        \n",
    "        results = {\n",
    "            'accuracy': accuracy,\n",
    "            'classification_report': report,\n",
    "            'training_time': self.training_time,\n",
    "            'inference_time_per_sample': inference_time,\n",
    "            'predictions': predictions,\n",
    "            'model_name': 'SVM + HOG'\n",
    "        }\n",
    "        \n",
    "        print(f\"üìà SVM + HOG Results:\")\n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  Training Time: {self.training_time:.2f} seconds\")\n",
    "        print(f\"  Inference Time: {inference_time:.6f} seconds per sample\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def measure_inference_time(self, X_sample, num_runs=5):\n",
    "        \"\"\"Measure detailed inference time\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained before measuring inference time\")\n",
    "        \n",
    "        times = []\n",
    "        for _ in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            _ = self.predict(X_sample)\n",
    "            end_time = time.time()\n",
    "            times.append((end_time - start_time) / len(X_sample))\n",
    "        \n",
    "        return {\n",
    "            'mean_time': np.mean(times),\n",
    "            'std_time': np.std(times),\n",
    "            'min_time': np.min(times),\n",
    "            'max_time': np.max(times)\n",
    "        }\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        \"\"\"Get model configuration information\"\"\"\n",
    "        return {\n",
    "            'model_type': 'SVM + HOG',\n",
    "            'hog_params': self.hog_params,\n",
    "            'svm_params': self.svm_params,\n",
    "            'is_trained': self.is_trained,\n",
    "            'training_time': self.training_time\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ SVM + HOG model class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnn_model"
   },
   "source": [
    "## üß† CNN Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cnn_model_class"
   },
   "outputs": [],
   "source": [
    "# CNN Model Class\n",
    "class CNNModel:\n",
    "    def __init__(self, num_classes, config=None):\n",
    "        self.config = config or CNN_CONFIG\n",
    "        self.num_classes = num_classes\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.is_trained = False\n",
    "        self.training_time = 0\n",
    "        \n",
    "        # Build model\n",
    "        self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        \"\"\"Build CNN architecture\"\"\"\n",
    "        print(\"üèóÔ∏è Building CNN model...\")\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        \n",
    "        # Input layer\n",
    "        input_shape = self.config['architecture']['input_shape']\n",
    "        \n",
    "        # Convolutional layers\n",
    "        for i, conv_config in enumerate(self.config['architecture']['conv_layers']):\n",
    "            if i == 0:\n",
    "                # First layer needs input shape\n",
    "                self.model.add(Conv2D(\n",
    "                    filters=conv_config['filters'],\n",
    "                    kernel_size=conv_config['kernel_size'],\n",
    "                    activation=conv_config['activation'],\n",
    "                    input_shape=input_shape\n",
    "                ))\n",
    "            else:\n",
    "                self.model.add(Conv2D(\n",
    "                    filters=conv_config['filters'],\n",
    "                    kernel_size=conv_config['kernel_size'],\n",
    "                    activation=conv_config['activation']\n",
    "                ))\n",
    "            \n",
    "            # Add pooling after each conv layer\n",
    "            self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        # Flatten before dense layers\n",
    "        self.model.add(Flatten())\n",
    "        \n",
    "        # Dense layers\n",
    "        for dense_config in self.config['architecture']['dense_layers']:\n",
    "            self.model.add(Dense(\n",
    "                units=dense_config['units'],\n",
    "                activation=dense_config['activation']\n",
    "            ))\n",
    "            \n",
    "            # Add dropout if specified\n",
    "            if 'dropout' in dense_config:\n",
    "                self.model.add(Dropout(dense_config['dropout']))\n",
    "        \n",
    "        # Output layer\n",
    "        self.model.add(Dense(\n",
    "            units=self.num_classes,\n",
    "            activation=self.config['architecture']['output_activation']\n",
    "        ))\n",
    "        \n",
    "        # Compile model\n",
    "        optimizer = Adam(learning_rate=self.config['compilation']['learning_rate'])\n",
    "        \n",
    "        self.model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=self.config['compilation']['loss'],\n",
    "            metrics=self.config['compilation']['metrics']\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ CNN model built successfully!\")\n",
    "        print(f\"üìä Total parameters: {self.model.count_params():,}\")\n",
    "        \n",
    "        # Display model summary\n",
    "        self.model.summary()\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None, data_generator=None):\n",
    "        \"\"\"Train the CNN model\"\"\"\n",
    "        print(\"üß† Training CNN model...\")\n",
    "        \n",
    "        # Setup callbacks\n",
    "        callbacks = []\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping_config = self.config['training']['early_stopping']\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor=early_stopping_config['monitor'],\n",
    "            patience=early_stopping_config['patience'],\n",
    "            restore_best_weights=early_stopping_config['restore_best_weights']\n",
    "        )\n",
    "        callbacks.append(early_stopping)\n",
    "        \n",
    "        # Training parameters\n",
    "        epochs = self.config['training']['epochs']\n",
    "        batch_size = self.config['training']['batch_size']\n",
    "        validation_split = self.config['training']['validation_split']\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        if data_generator is not None:\n",
    "            # Train with data augmentation\n",
    "            print(\"üîÑ Training with data augmentation...\")\n",
    "            \n",
    "            # Calculate steps\n",
    "            steps_per_epoch = len(X_train) // batch_size\n",
    "            \n",
    "            # Validation data\n",
    "            if X_val is not None and y_val is not None:\n",
    "                validation_data = (X_val, y_val)\n",
    "            else:\n",
    "                validation_data = None\n",
    "            \n",
    "            self.history = self.model.fit(\n",
    "                data_generator,\n",
    "                epochs=epochs,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "                validation_data=validation_data,\n",
    "                callbacks=callbacks,\n",
    "                verbose=1\n",
    "            )\n",
    "        else:\n",
    "            # Train without data augmentation\n",
    "            print(\"üìö Training without data augmentation...\")\n",
    "            \n",
    "            self.history = self.model.fit(\n",
    "                X_train, y_train,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                validation_split=validation_split,\n",
    "                callbacks=callbacks,\n",
    "                verbose=1\n",
    "            )\n",
    "        \n",
    "        self.training_time = time.time() - start_time\n",
    "        self.is_trained = True\n",
    "        \n",
    "        print(f\"‚úÖ Training completed in {self.training_time:.2f} seconds\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained before making predictions\")\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = self.model.predict(X_test)\n",
    "        \n",
    "        # Convert probabilities to class predictions\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        return predicted_classes\n",
    "    \n",
    "    def predict_proba(self, X_test):\n",
    "        \"\"\"Get prediction probabilities\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained before making predictions\")\n",
    "        \n",
    "        probabilities = self.model.predict(X_test)\n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def evaluate(self, X_test, y_test, class_names):\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        print(\"üìä Evaluating CNN model...\")\n",
    "        \n",
    "        # Make predictions\n",
    "        start_time = time.time()\n",
    "        predictions = self.predict(X_test)\n",
    "        inference_time = (time.time() - start_time) / len(X_test)\n",
    "        \n",
    "        # Convert categorical y_test to class indices if needed\n",
    "        if len(y_test.shape) > 1:\n",
    "            y_test_classes = np.argmax(y_test, axis=1)\n",
    "        else:\n",
    "            y_test_classes = y_test\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test_classes, predictions)\n",
    "        \n",
    "        # Generate classification report\n",
    "        report = classification_report(\n",
    "            y_test_classes, predictions, \n",
    "            target_names=class_names, \n",
    "            output_dict=True\n",
    "        )\n",
    "        \n",
    "        results = {\n",
    "            'accuracy': accuracy,\n",
    "            'classification_report': report,\n",
    "            'training_time': self.training_time,\n",
    "            'inference_time_per_sample': inference_time,\n",
    "            'predictions': predictions,\n",
    "            'model_name': 'CNN',\n",
    "            'history': self.history.history if self.history else None\n",
    "        }\n",
    "        \n",
    "        print(f\"üìà CNN Results:\")\n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  Training Time: {self.training_time:.2f} seconds\")\n",
    "        print(f\"  Inference Time: {inference_time:.6f} seconds per sample\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def measure_inference_time(self, X_sample, num_runs=5):\n",
    "        \"\"\"Measure detailed inference time\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained before measuring inference time\")\n",
    "        \n",
    "        times = []\n",
    "        for _ in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            _ = self.predict(X_sample)\n",
    "            end_time = time.time()\n",
    "            times.append((end_time - start_time) / len(X_sample))\n",
    "        \n",
    "        return {\n",
    "            'mean_time': np.mean(times),\n",
    "            'std_time': np.std(times),\n",
    "            'min_time': np.min(times),\n",
    "            'max_time': np.max(times)\n",
    "        }\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        \"\"\"Get model configuration information\"\"\"\n",
    "        return {\n",
    "            'model_type': 'CNN',\n",
    "            'architecture': self.config['architecture'],\n",
    "            'compilation': self.config['compilation'],\n",
    "            'training': self.config['training'],\n",
    "            'augmentation': self.config['augmentation'],\n",
    "            'is_trained': self.is_trained,\n",
    "            'training_time': self.training_time,\n",
    "            'num_parameters': self.model.count_params() if self.model else 0\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ CNN model class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## üöÄ Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_svm"
   },
   "outputs": [],
   "source": [
    "# Train SVM + HOG Model\n",
    "print(\"üîß Training SVM + HOG Model...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "svm_model = SVMHOGModel(config=SVM_HOG_CONFIG)\n",
    "svm_model.train(X_train_svm, y_train_svm)\n",
    "\n",
    "print(\"\\n‚úÖ SVM + HOG model training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_cnn"
   },
   "outputs": [],
   "source": [
    "# Train CNN Model\n",
    "print(\"üß† Training CNN Model...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "cnn_model = CNNModel(num_classes=len(class_names), config=CNN_CONFIG)\n",
    "\n",
    "# Setup data augmentation\n",
    "print(\"üîÑ Setting up data augmentation...\")\n",
    "train_generator = data_loader.create_data_generator(\n",
    "    X_train_cnn, y_train_cnn, \n",
    "    batch_size=CNN_CONFIG['training']['batch_size']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "cnn_model.train(X_train_cnn, y_train_cnn, data_generator=train_generator)\n",
    "\n",
    "print(\"\\n‚úÖ CNN model training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_training_history"
   },
   "outputs": [],
   "source": [
    "# Plot CNN Training History\n",
    "if cnn_model.history:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot training & validation accuracy\n",
    "    ax1.plot(cnn_model.history.history['accuracy'], label='Training Accuracy')\n",
    "    if 'val_accuracy' in cnn_model.history.history:\n",
    "        ax1.plot(cnn_model.history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax1.set_title('CNN Model Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot training & validation loss\n",
    "    ax2.plot(cnn_model.history.history['loss'], label='Training Loss')\n",
    "    if 'val_loss' in cnn_model.history.history:\n",
    "        ax2.plot(cnn_model.history.history['val_loss'], label='Validation Loss')\n",
    "    ax2.set_title('CNN Model Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìà CNN training history visualized!\")\nelse:\n",
    "    print(\"‚ö†Ô∏è No training history available for CNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## üìä Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate_models"
   },
   "outputs": [],
   "source": [
    "# Evaluate Both Models\n",
    "print(\"üìä Evaluating both models...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Evaluate SVM\n",
    "svm_results = svm_model.evaluate(X_test_svm, y_test_svm, class_names)\n",
    "svm_results['y_true'] = y_test_svm  # Store true labels for confusion matrix\n",
    "\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "\n",
    "# Evaluate CNN\n",
    "cnn_results = cnn_model.evaluate(X_test_cnn, y_test_cnn, class_names)\n",
    "# Convert categorical labels back to indices for confusion matrix\n",
    "if len(y_test_cnn.shape) > 1:\n",
    "    cnn_results['y_true'] = np.argmax(y_test_cnn, axis=1)\n",
    "else:\n",
    "    cnn_results['y_true'] = y_test_cnn\n",
    "\n",
    "print(\"\\n‚úÖ Model evaluation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "detailed_timing"
   },
   "outputs": [],
   "source": [
    "# Detailed Timing Analysis\n",
    "print(\"‚è±Ô∏è Detailed timing analysis...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Sample for inference timing\n",
    "sample_size = min(TIMING_CONFIG['inference_samples'], len(X_test_svm))\n",
    "X_sample_svm = X_test_svm[:sample_size]\n",
    "X_sample_cnn = X_test_cnn[:sample_size]\n",
    "\n",
    "# Measure inference times\n",
    "print(f\"üìä Measuring inference times on {sample_size} samples...\")\n",
    "svm_timing = svm_model.measure_inference_time(X_sample_svm, TIMING_CONFIG['timing_runs'])\n",
    "cnn_timing = cnn_model.measure_inference_time(X_sample_cnn, TIMING_CONFIG['timing_runs'])\n",
    "\n",
    "print(f\"\\nüîß SVM Inference Timing (avg of {TIMING_CONFIG['timing_runs']} runs):\")\n",
    "print(f\"  Mean: {svm_timing['mean_time']*1000:.3f} ms/sample\")\n",
    "print(f\"  Std:  {svm_timing['std_time']*1000:.3f} ms/sample\")\n",
    "print(f\"  Min:  {svm_timing['min_time']*1000:.3f} ms/sample\")\n",
    "print(f\"  Max:  {svm_timing['max_time']*1000:.3f} ms/sample\")\n",
    "\n",
    "print(f\"\\nüß† CNN Inference Timing (avg of {TIMING_CONFIG['timing_runs']} runs):\")\n",
    "print(f\"  Mean: {cnn_timing['mean_time']*1000:.3f} ms/sample\")\n",
    "print(f\"  Std:  {cnn_timing['std_time']*1000:.3f} ms/sample\")\n",
    "print(f\"  Min:  {cnn_timing['min_time']*1000:.3f} ms/sample\")\n",
    "print(f\"  Max:  {cnn_timing['max_time']*1000:.3f} ms/sample\")\n",
    "\n",
    "# Update results with detailed timing\n",
    "svm_results.update(svm_timing)\n",
    "cnn_results.update(cnn_timing)\n",
    "\n",
    "print(\"\\n‚úÖ Detailed timing analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualization"
   },
   "source": [
    "## üìà Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "comparison_visualizations"
   },
   "outputs": [],
   "source": [
    "# Model Comparison Visualizations\n",
    "class ModelEvaluator:\n",
    "    def __init__(self):\n",
    "        # Set plotting style\n",
    "        plt.style.use('default')\n",
    "        sns.set_palette(\"husl\")\n",
    "    \n",
    "    def plot_accuracy_comparison(self, svm_results, cnn_results, class_names):\n",
    "        \"\"\"Plot accuracy comparison\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Overall accuracy comparison\n",
    "        models = ['SVM + HOG', 'CNN']\n",
    "        accuracies = [svm_results['accuracy'], cnn_results['accuracy']]\n",
    "        \n",
    "        colors = ['#3498db', '#e74c3c']\n",
    "        bars = ax1.bar(models, accuracies, color=colors, alpha=0.7)\n",
    "        ax1.set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.set_ylim(0, 1)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, acc in zip(bars, accuracies):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Per-class accuracy comparison\n",
    "        svm_report = svm_results['classification_report']\n",
    "        cnn_report = cnn_results['classification_report']\n",
    "        \n",
    "        x = np.arange(len(class_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        svm_class_acc = [svm_report[class_name]['precision'] for class_name in class_names]\n",
    "        cnn_class_acc = [cnn_report[class_name]['precision'] for class_name in class_names]\n",
    "        \n",
    "        ax2.bar(x - width/2, svm_class_acc, width, label='SVM + HOG', color='#3498db', alpha=0.7)\n",
    "        ax2.bar(x + width/2, cnn_class_acc, width, label='CNN', color='#e74c3c', alpha=0.7)\n",
    "        \n",
    "        ax2.set_title('Per-Class Precision Comparison', fontsize=14, fontweight='bold')\n",
    "        ax2.set_ylabel('Precision')\n",
    "        ax2.set_xlabel('Classes')\n",
    "        ax2.set_xticks(x)\n",
    "        ax2.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_time_comparisons(self, svm_results, cnn_results):\n",
    "        \"\"\"Plot training and inference time comparisons\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        models = ['SVM + HOG', 'CNN']\n",
    "        \n",
    "        # Training time comparison\n",
    "        training_times = [svm_results['training_time'], cnn_results['training_time']]\n",
    "        \n",
    "        colors = ['#3498db', '#e74c3c']\n",
    "        bars1 = ax1.bar(models, training_times, color=colors, alpha=0.7)\n",
    "        \n",
    "        ax1.set_title('Training Time Comparison', fontsize=14, fontweight='bold')\n",
    "        ax1.set_ylabel('Training Time (seconds)')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, time_val in zip(bars1, training_times):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + max(training_times) * 0.01,\n",
    "                    f'{time_val:.2f}s', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Inference time comparison\n",
    "        inference_times = [svm_results['inference_time_per_sample'] * 1000, \n",
    "                         cnn_results['inference_time_per_sample'] * 1000]  # Convert to ms\n",
    "        \n",
    "        bars2 = ax2.bar(models, inference_times, color=colors, alpha=0.7)\n",
    "        \n",
    "        ax2.set_title('Inference Time Comparison', fontsize=14, fontweight='bold')\n",
    "        ax2.set_ylabel('Inference Time per Sample (ms)')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, time_val in zip(bars2, inference_times):\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + max(inference_times) * 0.01,\n",
    "                    f'{time_val:.3f}ms', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_confusion_matrices(self, svm_results, cnn_results, class_names):\n",
    "        \"\"\"Plot confusion matrices for both models\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # SVM Confusion Matrix\n",
    "        svm_cm = confusion_matrix(svm_results['y_true'], svm_results['predictions'])\n",
    "        sns.heatmap(svm_cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=class_names, yticklabels=class_names,\n",
    "                   ax=axes[0])\n",
    "        axes[0].set_title('SVM + HOG Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_xlabel('Predicted')\n",
    "        axes[0].set_ylabel('Actual')\n",
    "        \n",
    "        # CNN Confusion Matrix\n",
    "        cnn_cm = confusion_matrix(cnn_results['y_true'], cnn_results['predictions'])\n",
    "        sns.heatmap(cnn_cm, annot=True, fmt='d', cmap='Reds', \n",
    "                   xticklabels=class_names, yticklabels=class_names,\n",
    "                   ax=axes[1])\n",
    "        axes[1].set_title('CNN Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_xlabel('Predicted')\n",
    "        axes[1].set_ylabel('Actual')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_detailed_metrics(self, svm_results, cnn_results, class_names):\n",
    "        \"\"\"Plot detailed metrics comparison\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        svm_report = svm_results['classification_report']\n",
    "        cnn_report = cnn_results['classification_report']\n",
    "        \n",
    "        metrics = ['precision', 'recall', 'f1-score']\n",
    "        \n",
    "        for idx, metric in enumerate(metrics):\n",
    "            row = idx // 2\n",
    "            col = idx % 2\n",
    "            \n",
    "            x = np.arange(len(class_names))\n",
    "            width = 0.35\n",
    "            \n",
    "            svm_values = [svm_report[class_name][metric] for class_name in class_names]\n",
    "            cnn_values = [cnn_report[class_name][metric] for class_name in class_names]\n",
    "            \n",
    "            axes[row, col].bar(x - width/2, svm_values, width, label='SVM + HOG', \n",
    "                             color='#3498db', alpha=0.7)\n",
    "            axes[row, col].bar(x + width/2, cnn_values, width, label='CNN', \n",
    "                             color='#e74c3c', alpha=0.7)\n",
    "            \n",
    "            axes[row, col].set_title(f'{metric.capitalize()} Comparison', fontsize=12, fontweight='bold')\n",
    "            axes[row, col].set_ylabel(metric.capitalize())\n",
    "            axes[row, col].set_xlabel('Classes')\n",
    "            axes[row, col].set_xticks(x)\n",
    "            axes[row, col].set_xticklabels(class_names, rotation=45, ha='right')\n",
    "            axes[row, col].legend()\n",
    "            axes[row, col].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Overall metrics comparison\n",
    "        overall_metrics = ['macro avg', 'weighted avg']\n",
    "        \n",
    "        x = np.arange(len(overall_metrics))\n",
    "        width = 0.35\n",
    "        \n",
    "        svm_vals = [svm_report[om]['precision'] for om in overall_metrics if om in svm_report]\n",
    "        cnn_vals = [cnn_report[om]['precision'] for om in overall_metrics if om in cnn_report]\n",
    "        \n",
    "        if svm_vals and cnn_vals:\n",
    "            axes[1, 1].bar(x - width/2, svm_vals, width, label='SVM + HOG', \n",
    "                          color='#3498db', alpha=0.7)\n",
    "            axes[1, 1].bar(x + width/2, cnn_vals, width, label='CNN', \n",
    "                          color='#e74c3c', alpha=0.7)\n",
    "        \n",
    "        axes[1, 1].set_title('Overall Metrics Comparison', fontsize=12, fontweight='bold')\n",
    "        axes[1, 1].set_ylabel('Precision')\n",
    "        axes[1, 1].set_xlabel('Metric Type')\n",
    "        axes[1, 1].set_xticks(x)\n",
    "        axes[1, 1].set_xticklabels(overall_metrics, rotation=45, ha='right')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create evaluator and generate visualizations\n",
    "evaluator = ModelEvaluator()\n",
    "\n",
    "print(\"üìà Generating comparison visualizations...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Generate all visualizations\n",
    "evaluator.plot_accuracy_comparison(svm_results, cnn_results, class_names)\n",
    "evaluator.plot_time_comparisons(svm_results, cnn_results)\n",
    "evaluator.plot_confusion_matrices(svm_results, cnn_results, class_names)\n",
    "evaluator.plot_detailed_metrics(svm_results, cnn_results, class_names)\n",
    "\n",
    "print(\"\\n‚úÖ All visualizations generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "comparison_summary"
   },
   "source": [
    "## üìã Comprehensive Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "print_summary"
   },
   "outputs": [],
   "source": [
    "# Generate Comprehensive Comparison Summary\n",
    "def print_comparison_summary(svm_results, cnn_results):\n",
    "    \"\"\"Print comprehensive comparison summary\"\"\"\n",
    "    \n",
    "    # Calculate comparison metrics\n",
    "    accuracy_diff = cnn_results['accuracy'] - svm_results['accuracy']\n",
    "    training_time_ratio = cnn_results['training_time'] / svm_results['training_time']\n",
    "    inference_time_ratio = cnn_results['inference_time_per_sample'] / svm_results['inference_time_per_sample']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üèÜ COMPREHENSIVE MODEL COMPARISON SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nüìä ACCURACY COMPARISON:\")\n",
    "    print(f\"  SVM + HOG: {svm_results['accuracy']:.4f}\")\n",
    "    print(f\"  CNN:       {cnn_results['accuracy']:.4f}\")\n",
    "    print(f\"  Difference: {accuracy_diff:.4f} {'(CNN better)' if accuracy_diff > 0 else '(SVM better)'}\")\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è  TRAINING TIME COMPARISON:\")\n",
    "    print(f\"  SVM + HOG: {svm_results['training_time']:.2f} seconds\")\n",
    "    print(f\"  CNN:       {cnn_results['training_time']:.2f} seconds\")\n",
    "    print(f\"  Ratio:     {training_time_ratio:.2f}x {'(CNN slower)' if training_time_ratio > 1 else '(CNN faster)'}\")\n",
    "    \n",
    "    print(f\"\\nüöÄ INFERENCE TIME COMPARISON:\")\n",
    "    print(f\"  SVM + HOG: {svm_results['inference_time_per_sample']*1000:.3f} ms/sample\")\n",
    "    print(f\"  CNN:       {cnn_results['inference_time_per_sample']*1000:.3f} ms/sample\")\n",
    "    print(f\"  Ratio:     {inference_time_ratio:.2f}x {'(CNN slower)' if inference_time_ratio > 1 else '(CNN faster)'}\")\n",
    "    \n",
    "    print(f\"\\nüèÜ RECOMMENDATIONS:\")\n",
    "    if accuracy_diff > 0:\n",
    "        print(\"  ‚Ä¢ CNN achieves higher accuracy\")\n",
    "    else:\n",
    "        print(\"  ‚Ä¢ SVM + HOG achieves higher accuracy\")\n",
    "    \n",
    "    if training_time_ratio > 1:\n",
    "        print(\"  ‚Ä¢ SVM + HOG trains faster\")\n",
    "    else:\n",
    "        print(\"  ‚Ä¢ CNN trains faster\")\n",
    "    \n",
    "    if inference_time_ratio > 1:\n",
    "        print(\"  ‚Ä¢ SVM + HOG has faster inference\")\n",
    "    else:\n",
    "        print(\"  ‚Ä¢ CNN has faster inference\")\n",
    "    \n",
    "    # Overall recommendation\n",
    "    print(f\"\\nüí° OVERALL RECOMMENDATION:\")\n",
    "    if accuracy_diff > 0.05:  # CNN significantly better\n",
    "        print(\"  ‚Ä¢ Use CNN for better accuracy (recommended for this dataset)\")\n",
    "    elif accuracy_diff < -0.05:  # SVM significantly better\n",
    "        print(\"  ‚Ä¢ Use SVM + HOG for better accuracy (recommended for this dataset)\")\n",
    "    else:\n",
    "        if training_time_ratio > 2:  # SVM much faster to train\n",
    "            print(\"  ‚Ä¢ Use SVM + HOG for faster training with comparable accuracy\")\n",
    "        else:\n",
    "            print(\"  ‚Ä¢ Choice depends on your priority: accuracy vs. speed\")\n",
    "    \n",
    "    print(\"\\nüìö KEY INSIGHTS:\")\n",
    "    print(\"  ‚Ä¢ SVM + HOG: Traditional approach, faster training, good for small datasets\")\n",
    "    print(\"  ‚Ä¢ CNN: Deep learning approach, potentially higher accuracy, better for large datasets\")\n",
    "    print(\"  ‚Ä¢ Data augmentation can significantly improve CNN performance\")\n",
    "    print(\"  ‚Ä¢ HOG features capture edge information effectively\")\n",
    "    print(\"  ‚Ä¢ CNN learns features automatically from data\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Print comprehensive summary\n",
    "print_comparison_summary(svm_results, cnn_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_results"
   },
   "source": [
    "## üíæ Save Results and Model Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_model_info"
   },
   "outputs": [],
   "source": [
    "# Save Model Information and Results\n",
    "print(\"üíæ Saving model information and results...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create results directory\n",
    "results_dir = 'results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Prepare comprehensive model information\n",
    "model_info = {\n",
    "    'experiment_info': {\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'gpu_used': len(tf.config.experimental.list_physical_devices('GPU')) > 0,\n",
    "        'tensorflow_version': tf.__version__,\n",
    "        'total_experiment_time': svm_results['training_time'] + cnn_results['training_time']\n",
    "    },\n",
    "    'dataset_info': {\n",
    "        'num_classes': len(class_names),\n",
    "        'class_names': class_names,\n",
    "        'total_samples': len(X),\n",
    "        'train_samples': len(X_train_svm),\n",
    "        'test_samples': len(X_test_svm),\n",
    "        'image_size': DATASET_CONFIG['image_size'],\n",
    "        'test_size_ratio': DATASET_CONFIG['test_size']\n",
    "    },\n",
    "    'svm_model': {\n",
    "        'model_info': svm_model.get_model_info(),\n",
    "        'results': {\n",
    "            'accuracy': svm_results['accuracy'],\n",
    "            'training_time': svm_results['training_time'],\n",
    "            'inference_time_per_sample': svm_results['inference_time_per_sample'],\n",
    "            'mean_inference_time': svm_results['mean_time'],\n",
    "            'std_inference_time': svm_results['std_time']\n",
    "        }\n",
    "    },\n",
    "    'cnn_model': {\n",
    "        'model_info': cnn_model.get_model_info(),\n",
    "        'results': {\n",
    "            'accuracy': cnn_results['accuracy'],\n",
    "            'training_time': cnn_results['training_time'],\n",
    "            'inference_time_per_sample': cnn_results['inference_time_per_sample'],\n",
    "            'mean_inference_time': cnn_results['mean_time'],\n",
    "            'std_inference_time': cnn_results['std_time']\n",
    "        }\n",
    "    },\n",
    "    'comparison_summary': {\n",
    "        'accuracy_difference': cnn_results['accuracy'] - svm_results['accuracy'],\n",
    "        'training_time_ratio': cnn_results['training_time'] / svm_results['training_time'],\n",
    "        'inference_time_ratio': cnn_results['inference_time_per_sample'] / svm_results['inference_time_per_sample'],\n",
    "        'winner_accuracy': 'CNN' if cnn_results['accuracy'] > svm_results['accuracy'] else 'SVM',\n",
    "        'winner_training_speed': 'SVM' if svm_results['training_time'] < cnn_results['training_time'] else 'CNN',\n",
    "        'winner_inference_speed': 'SVM' if svm_results['inference_time_per_sample'] < cnn_results['inference_time_per_sample'] else 'CNN'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save model information as JSON\n",
    "with open(os.path.join(results_dir, 'model_comparison.json'), 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Model information saved to {results_dir}/model_comparison.json\")\n",
    "\n",
    "# Save classification reports\n",
    "with open(os.path.join(results_dir, 'svm_classification_report.json'), 'w') as f:\n",
    "    json.dump(svm_results['classification_report'], f, indent=2)\n",
    "\n",
    "with open(os.path.join(results_dir, 'cnn_classification_report.json'), 'w') as f:\n",
    "    json.dump(cnn_results['classification_report'], f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Classification reports saved\")\n",
    "\n",
    "# Save CNN training history if available\n",
    "if cnn_model.history:\n",
    "    with open(os.path.join(results_dir, 'cnn_training_history.json'), 'w') as f:\n",
    "        json.dump(cnn_model.history.history, f, indent=2)\n",
    "    print(f\"‚úÖ CNN training history saved\")\n",
    "\n",
    "print(\"\\nüìÅ All results saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_section"
   },
   "source": [
    "## üì• Download Results and Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_functions"
   },
   "outputs": [],
   "source": [
    "# Download Functions\n",
    "from google.colab import files\n",
    "import pickle\n",
    "\n",
    "def create_download_packages():\n",
    "    \"\"\"Create separate download packages for different components\"\"\"\n",
    "    \n",
    "    print(\"üì¶ Creating download packages...\")\n",
    "    \n",
    "    # 1. Results Package\n",
    "    results_files = [\n",
    "        'results/model_comparison.json',\n",
    "        'results/svm_classification_report.json',\n",
    "        'results/cnn_classification_report.json'\n",
    "    ]\n",
    "    \n",
    "    if os.path.exists('results/cnn_training_history.json'):\n",
    "        results_files.append('results/cnn_training_history.json')\n",
    "    \n",
    "    with zipfile.ZipFile('results_package.zip', 'w') as zipf:\n",
    "        for file in results_files:\n",
    "            if os.path.exists(file):\n",
    "                zipf.write(file, os.path.basename(file))\n",
    "    \n",
    "    # 2. Models Package\n",
    "    print(\"üíæ Saving trained models...\")\n",
    "    \n",
    "    # Save SVM model\n",
    "    with open('svm_model.pkl', 'wb') as f:\n",
    "        pickle.dump(svm_model, f)\n",
    "    \n",
    "    # Save CNN model\n",
    "    cnn_model.model.save('cnn_model.h5')\n",
    "    \n",
    "    # Create models zip\n",
    "    with zipfile.ZipFile('models_package.zip', 'w') as zipf:\n",
    "        zipf.write('svm_model.pkl')\n",
    "        zipf.write('cnn_model.h5')\n",
    "    \n",
    "    # 3. Dataset Package\n",
    "    print(\"üìä Packaging dataset...\")\n",
    "    \n",
    "    with zipfile.ZipFile('dataset_package.zip', 'w') as zipf:\n",
    "        for root, dirs, files in os.walk('dataset'):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, 'dataset')\n",
    "                zipf.write(file_path, f'dataset/{arcname}')\n",
    "    \n",
    "    # 4. Configuration Package\n",
    "    config_data = {\n",
    "        'DATASET_CONFIG': DATASET_CONFIG,\n",
    "        'SVM_HOG_CONFIG': SVM_HOG_CONFIG,\n",
    "        'CNN_CONFIG': CNN_CONFIG,\n",
    "        'EVALUATION_CONFIG': EVALUATION_CONFIG,\n",
    "        'TIMING_CONFIG': TIMING_CONFIG\n",
    "    }\n",
    "    \n",
    "    with open('configurations.json', 'w') as f:\n",
    "        json.dump(config_data, f, indent=2)\n",
    "    \n",
    "    with zipfile.ZipFile('config_package.zip', 'w') as zipf:\n",
    "        zipf.write('configurations.json')\n",
    "    \n",
    "    print(\"‚úÖ All download packages created!\")\n",
    "    \n",
    "    return {\n",
    "        'results': 'results_package.zip',\n",
    "        'models': 'models_package.zip',\n",
    "        'dataset': 'dataset_package.zip',\n",
    "        'config': 'config_package.zip'\n",
    "    }\n",
    "\n",
    "# Create download packages\n",
    "download_packages = create_download_packages()\n",
    "\n",
    "print(\"\\nüì• Download packages ready!\")\n",
    "print(\"Click on the links below to download:\")\n",
    "for package_type, filename in download_packages.items():\n",
    "    print(f\"  ‚Ä¢ {package_type.capitalize()}: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_results"
   },
   "outputs": [],
   "source": [
    "# Download Results Package\n",
    "print(\"üì• Downloading Results Package...\")\n",
    "files.download('results_package.zip')\nprint(\"‚úÖ Results package downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_models"
   },
   "outputs": [],
   "source": [
    "# Download Models Package\n",
    "print(\"üì• Downloading Models Package...\")\n",
    "files.download('models_package.zip')\nprint(\"‚úÖ Models package downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_dataset"
   },
   "outputs": [],
   "source": [
    "# Download Dataset Package\n",
    "print(\"üì• Downloading Dataset Package...\")\n",
    "files.download('dataset_package.zip')\nprint(\"‚úÖ Dataset package downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_config"
   },
   "outputs": [],
   "source": [
    "# Download Configuration Package\n",
    "print(\"üì• Downloading Configuration Package...\")\n",
    "files.download('config_package.zip')\nprint(\"‚úÖ Configuration package downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructions"
   },
   "source": [
    "## üìã Instructions for GitHub Repository Upload\n",
    "\n",
    "### üîÑ To upload the dataset to your exam repository:\n",
    "\n",
    "1. **Download the dataset package** using the cell above\n",
    "2. **Extract the dataset_package.zip** file\n",
    "3. **Upload the extracted 'dataset' folder** to your GitHub exam repository\n",
    "4. **Commit and push** the changes\n",
    "\n",
    "### üìÅ Repository Structure Recommendation:\n",
    "```\n",
    "your-exam-repo/\n",
    "‚îú‚îÄ‚îÄ dataset/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ normal/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ normal_001.png\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ normal_002.png\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ cheating/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cheating_001.png\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cheating_002.png\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ looking_around/\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ looking_around_001.png\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ looking_around_002.png\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ ...\n",
    "‚îú‚îÄ‚îÄ SVM_vs_CNN_Image_Classification_Comparison.ipynb\n",
    "‚îú‚îÄ‚îÄ README.md\n",
    "‚îî‚îÄ‚îÄ results/\n",
    "```\n",
    "\n",
    "### üöÄ Usage Instructions:\n",
    "\n",
    "1. **Upload this notebook** to your GitHub repository\n",
    "2. **Open in Google Colab** by clicking the \"Open in Colab\" button\n",
    "3. **Run all cells** to reproduce the complete comparison\n",
    "4. **Download results** using the download cells above\n",
    "\n",
    "### üìä What you get:\n",
    "\n",
    "- **Complete comparison** between SVM+HOG and CNN approaches\n",
    "- **Comprehensive visualizations** and performance metrics\n",
    "- **Trained models** ready for deployment\n",
    "- **Detailed analysis** and recommendations\n",
    "- **Reproducible results** with consistent random seeds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "final_summary"
   },
   "source": [
    "## üéØ Final Summary\n",
    "\n",
    "### ‚úÖ Experiment Completed Successfully!\n",
    "\n",
    "This notebook has successfully:\n",
    "\n",
    "1. **üîç Created a synthetic dataset** with 3 classes and distinct visual patterns\n",
    "2. **üèóÔ∏è Implemented two different approaches:**\n",
    "   - SVM with HOG features (traditional computer vision)\n",
    "   - CNN with data augmentation (deep learning)\n",
    "3. **üìä Conducted comprehensive comparison** including:\n",
    "   - Accuracy comparison\n",
    "   - Training time analysis\n",
    "   - Inference speed measurements\n",
    "   - Detailed visualizations\n",
    "4. **üéÆ Utilized GPU acceleration** when available\n",
    "5. **üì• Provided separate downloads** for all components\n",
    "6. **üìã Generated detailed reports** and recommendations\n",
    "\n",
    "### üèÜ Key Insights:\n",
    "\n",
    "- **SVM + HOG**: Fast training, good for small datasets, interpretable features\n",
    "- **CNN**: Potentially higher accuracy, automatic feature learning, better scalability\n",
    "- **GPU acceleration**: Significantly speeds up CNN training\n",
    "- **Data augmentation**: Improves CNN generalization\n",
    "\n",
    "### üìö Next Steps:\n",
    "\n",
    "1. **Upload dataset** to your GitHub exam repository\n",
    "2. **Experiment with different configurations** using the config cells\n",
    "3. **Try with your own dataset** by modifying the data loading section\n",
    "4. **Deploy models** for real-world applications\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations! You now have a complete image classification comparison system ready for Google Colab!**\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SVM_vs_CNN_Image_Classification_Comparison.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}